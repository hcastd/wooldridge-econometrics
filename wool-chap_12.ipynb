{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory Econometrics: A modern approach\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Serial Correlation and Heteroskedasticity in Time Series Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Book: \n",
    "\n",
    "    - WOOLDRIDGE, J. M.. Introdução à econometria: uma abordagem moderna. 3a ed. São Paulo: Pioneira Thomson Learning, 2006.\n",
    "\n",
    "    - WOOLDRIDGE, Jeffrey M. Introductory econometrics: A modern approach. Cengage learning, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LINK to codes correction:\n",
    "\n",
    "http://www.upfie.net/downloads12.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import patsy as pt\n",
    "\n",
    "from statsmodels.stats.diagnostic import acorr_breusch_godfrey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propriedades dos estimadores de MQO com correlação serial \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Viés e consistência:**\n",
    "\n",
    "- Assim como no caso da heteroscedasticidade, não teremos estimadores inconsistentes caso haja correlação serial: enquanto as variáveis forem de fato exógenas, os estimadores serão **consistentes;**\n",
    "\n",
    "- Contudo, pode haver viés em alguns casos: caso haja dependência fraca entre *x* e *u.*\n",
    "\n",
    "2. **Eficiência e inferência:**\n",
    "\n",
    "- Estimadores de MQO não são mais BLUE com correlação serial.;\n",
    "\n",
    "- Os erros padrões dos estimadores e as estatísticas de teste não são mais válidas, nem assintóticamente.\n",
    "\n",
    "- O estimador da variância estará sub ou sobre estimado, inviabilizando os testes estatísticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coeficiente de determinação ($R^2$)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Se a série for estacionária ou com dependência fraca, o coeficiente de determinação ainda é válido, dado que a variância do erro e da variável dependente não mudam muito ao longo do tempo.\n",
    "\n",
    "- Sem essas condições, ou com sua persistência mesmo com a série já diferenciada, a métrica perde seu sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlação serial com variáveis defasadas\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mesmo com correlação serial, é possível obter estimadores consistentes, mas, ainda terão as estatísticas de teste inválidas.\n",
    "\n",
    "- Os estimadores só se tornam **inconsistentes quando o erro também segue um processo autorregressivo.**\n",
    "\n",
    "## Obs 1:\n",
    "\n",
    "- Muitas vezes, a presença de correlação serial nada mais indica do que um modelo mal especificado, ou seja, acrescentar mais uma variável ou mais uma defasagem podem corrigir o problema, dependendo do caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando para a correlação serial\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Teste t para regressores estritamente exógenos\n",
    "\n",
    "- $H_0$: não há correlação serial ($\\rho_1 = 0$);\n",
    "\n",
    "- $H_1$: há correlação serial($\\rho_1 \\neq 0$).\n",
    "\n",
    "Devemos assumir que o valor esperado do erro, dado o histórico de variáveis independentes, é zero e que a variância é constante. \n",
    "\n",
    "Podemos observar e testar essa hipótese regredindo *u* em t em *u* em t-1 mais + um novo termo de erro *e*. Para esse teste, **usamos e estatística t**, já conhecida.\n",
    "\n",
    "$$\n",
    "u_t = \\rho u_{t-1} + e_t\n",
    "$$\n",
    "\n",
    "Como não podemos observar o u, usaremos u chapéu, os resíduos da regressão com os dados da nossa amostra.\n",
    "\n",
    "Em linhas gerais, tratamos a correlação serial como um **problema relevante** quando a Hipótese Nula é rejeitada em um nível de 5% (p-valor < 0.05, baixíssima probabilidade de errarmos ao rejeitar a hipótese nula).\n",
    "\n",
    "**VALE LEMBRAR:** Importância estatística X Importância prática\n",
    "\n",
    "- Mesmo se a hipótese nula for rejeitada e houver correlação serial, se seu valor estimado em for muito baixo, temos uma importância prática pouco relevante. Ou seja, usar MQO para estimar mesmo com correlação serial não nos traz grandes problemas práticos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 12.1: Testing for AR(1) Serial Correlation in the Phillips Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phillips Curve: expectations augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table: \n",
      "                    p      se       t    pval\n",
      "Intercept      0.1942  0.3004  0.6464  0.5213\n",
      "resid_ea_lag1 -0.0356  0.1239 -0.2873  0.7752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phillips = woo.dataWoo('phillips')\n",
    "T = len(phillips)\n",
    "\n",
    "# define yearly time series beginning in 1948:\n",
    "date_range = pd.date_range(start='1948', periods=T, freq='Y')\n",
    "phillips.index = date_range.year\n",
    "\n",
    "# estimation of expectations-augmented Phillips curve:\n",
    "yt96 = (phillips['year'] <= 1996)\n",
    "phillips['inf_diff1'] = phillips['inf'].diff()\n",
    "reg_ea = smf.ols(formula='inf_diff1 ~ unem', data=phillips, subset=yt96)\n",
    "results_ea = reg_ea.fit()\n",
    "\n",
    "phillips['resid_ea'] = results_ea.resid\n",
    "phillips['resid_ea_lag1'] = phillips['resid_ea'].shift(1)\n",
    "reg = smf.ols(formula='resid_ea ~ resid_ea_lag1', data=phillips, subset=yt96)\n",
    "results = reg.fit()\n",
    "\n",
    "# print regression table:\n",
    "table = pd.DataFrame({'p': round(results.params, 4),\n",
    "                      'se': round(results.bse, 4),\n",
    "                      't': round(results.tvalues, 4),\n",
    "                      'pval': round(results.pvalues, 4)})\n",
    "print(f'table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phillips Curve: static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table: \n",
      "                   p      se       t    pval\n",
      "Intercept    -0.1134  0.3594 -0.3155  0.7538\n",
      "resid_s_lag1  0.5730  0.1161  4.9337  0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phillips = woo.dataWoo('phillips')\n",
    "T = len(phillips)\n",
    "\n",
    "# define yearly time series beginning in 1948:\n",
    "date_range = pd.date_range(start='1948', periods=T, freq='Y')\n",
    "phillips.index = date_range.year\n",
    "\n",
    "# estimation of static Phillips curve:\n",
    "yt96 = (phillips['year'] <= 1996)\n",
    "reg_s = smf.ols(formula='Q(\"inf\") ~ unem', data=phillips, subset=yt96)\n",
    "results_s = reg_s.fit()\n",
    "\n",
    "# residuals and AR(1) test:\n",
    "phillips['resid_s'] = results_s.resid\n",
    "phillips['resid_s_lag1'] = phillips['resid_s'].shift(1)\n",
    "reg = smf.ols(formula='resid_s ~ resid_s_lag1', data=phillips, subset=yt96)\n",
    "results = reg.fit()\n",
    "\n",
    "# print regression table:\n",
    "table = pd.DataFrame({'p': round(results.params, 4),\n",
    "                      'se': round(results.bse, 4),\n",
    "                      't': round(results.tvalues, 4),\n",
    "                      'pval': round(results.pvalues, 4)})\n",
    "print(f'table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS:** Apesar dos exemplos com a curva de Phillips e com o modelo AR(1), esse teste serve para detectar a correlação em qualquer especificação de regressão em séries temporais, justamente pelo seu formato. \n",
    "\n",
    "$$\n",
    "u_t = \\rho u_{t-1} + e_t\n",
    "$$\n",
    "\n",
    "Mas não necessariamente irá detectar em todas as situações: pode ser que o erro não seja correlacionado com sua defasagem imediatamente anterior (erros adjacentes), mas que esteja correlacionado com defasagens mais antigas, por conta de sazonalidade, por exemplo. \n",
    "\n",
    "**OBS 2:** Em geral, **assumimos que há homoscedasticidade** no termo do erro para essa regressão. Mas caso essa hipótese não se encaixe para o caso, podemos simplesmente utilizar a **estatística t e os erros padrões robustos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Teste de Durbin-Watson (DW) sob as hipóteses clássicas\n",
    "\n",
    "- Teste também baseado nos **resíduos da regressão estimada com MQO.**\n",
    "\n",
    "- Conceitualmente muito similar ao teste t com regressores estritamente exógenos.\n",
    "\n",
    "- Esse teste demanda a “validade” (ou algo bem próximo) de todas as hipóteses clássicas para modelos de regressão linear em séries temporais, inclusive a normalidade dos erros.\n",
    "\n",
    "- Dessa forma, depende do: **tamanho da amostra, quantidade de regressores e se a regressão contém o intercepto.**\n",
    "\n",
    "- O conjunto de hipóteses é similar ao primeiro teste acima.\n",
    "\n",
    "- Limites superiores e inferiores para a estatística de teste DW e com possibilidade de inconclusividade no teste.\n",
    "\n",
    "- Apesar de possuir uma vantagem de computar a distribuição amostral da estatística DW é a única vantagem em relação ao método anterior. Mas suas dificuldades práticas podem deixar o primeiro teste como uma opção mais atrativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Teste para regressores sem exogeneidade estrita\n",
    "\n",
    "- Sem exogeneidae estrita, nem o teste t e nem o Durbin-Watson serão válidos, mesmo em grandes amostras.\n",
    "\n",
    "- Estatística alternativa de Durbin:\n",
    "    - Obter os resídos da regressão de y nos x;\n",
    "\n",
    "    - Regressão dos resíduos nos x e na primeira defasagem de u e obtenção do p e sua estatística de teste;\n",
    "\n",
    "    - Usar t para testar a hipótese como ocorre no teste t relatado acima\n",
    "\n",
    "- Incluir as variáveis explicativas nessa regressão explicita a relação direta que o resíduo e sua primeira defasagem podem ter com as variáveis exógenas, controlando para a endogeneidade das variáveis exógenas.\n",
    "\n",
    "- Vale lembrar que essa estatística pode ser válida sob heteroscedasticidade utilizando sua versão robusta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando para correlação serial de ordem superior\n",
    "\n",
    "- Os testes destacados acima podem facilmente ser estendidos para correlações seriais de ordem superior, ou seja, para mais defasagens no termo de erro\n",
    "\n",
    "- Tomamos os mesmo procedimentos, mas agora, como estamos testando para a correlação serial em mais defasagens, podemos usar também o teste F como referencial, além do teste t.\n",
    "\n",
    "- Há também uma versão do teste robusta a heteroscedasticidade\n",
    "\n",
    "- Outra alternativa de teste seria utilizar a estatística do Multiplicador de Lagrange, conhecida também por **Breusch-Godfrey test**.\n",
    "\n",
    "    - Esse teste também pode ser feito com robustes à heteroscedasticidade\n",
    "\n",
    "\n",
    "- Para séries sem ajuste sazonal, sejam mensais ou trimestrais, geralmente podemos usar essa mesma estrutura de teste para investigar as defasagens no período em frequência t que se espera a sazonalidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrigindo a correlação serial com regressores exógenos \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtendo o melhor estimador linear não viesado\n",
    "\n",
    "- Podemos usar os estimadores de mínimos quadrados generalizados (MQG)\n",
    "\n",
    "- Os estimadores de MQG, ou seja, de MQO nos dados tranformados, são diferentes dos inicialmente estimados por MQO\n",
    "\n",
    "- Estimadores de MQG se tornam BLUE e como os erros estão transformados, a equação não possui correlação serial e é homoscedastica;\n",
    "\n",
    "- Os testes t e F são válidos\n",
    "\n",
    "## Estimação factível de MQG\n",
    "\n",
    "1. Estimação via MQO e obtenção dos resíduos\n",
    "\n",
    "2. Regressão dos resíduos nas variáveis exógenas e em uma defasagem \n",
    "\n",
    "3. Aplicar a MQO para a equação com os dados transformados\n",
    "\n",
    "4. Com esse processo, os testes t e F são assintóticamente válidos\n",
    "\n",
    "- Assim, temos a estimação factível por MQG, o MQGF: mínimos quadrados generalizados factível\n",
    "\n",
    "- Os estimadores de MQGF não são não-viesados, mas são mais eficientes assintóticamente do que os estimadores de MQO.\n",
    "\n",
    "- Contudo, isso ocorre apenas se assumirmos que a série possui **dependência fraca**.\n",
    "\n",
    "- Diferentes formas de se obter estimadores MQGF:\n",
    "\n",
    "    - Cochrane-Orcutt (CO) estimation: omite a primeira observação\n",
    "\n",
    "    - Prais-Winsten (PW) estimation: usa a primeira observação.\n",
    "\n",
    "- Ambas as formas podem ser usadas de interativamente e repetimos as estimações até que os resultados sejam relativamente parecidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 12.4: Prais-Winsten Estimation in the Event Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chnimp</th>\n",
       "      <th>bchlimp</th>\n",
       "      <th>befile6</th>\n",
       "      <th>affile6</th>\n",
       "      <th>afdec6</th>\n",
       "      <th>befile12</th>\n",
       "      <th>affile12</th>\n",
       "      <th>afdec12</th>\n",
       "      <th>chempi</th>\n",
       "      <th>gas</th>\n",
       "      <th>...</th>\n",
       "      <th>apr</th>\n",
       "      <th>may</th>\n",
       "      <th>jun</th>\n",
       "      <th>jul</th>\n",
       "      <th>aug</th>\n",
       "      <th>sep</th>\n",
       "      <th>oct</th>\n",
       "      <th>nov</th>\n",
       "      <th>dec</th>\n",
       "      <th>percchn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>220.462006</td>\n",
       "      <td>9578.375977</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.099998</td>\n",
       "      <td>7.830000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.301664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94.797997</td>\n",
       "      <td>11219.480469</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.900002</td>\n",
       "      <td>8.820000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.844941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>219.357498</td>\n",
       "      <td>9719.900391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101.099998</td>\n",
       "      <td>8.450000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.256788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>317.421509</td>\n",
       "      <td>12920.950195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>102.500000</td>\n",
       "      <td>9.240001e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.456642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114.639000</td>\n",
       "      <td>9790.446289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>104.099998</td>\n",
       "      <td>9.150000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.170927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>589.421997</td>\n",
       "      <td>31686.859375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>154.800003</td>\n",
       "      <td>9.420000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.860147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>529.733521</td>\n",
       "      <td>16370.339844</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>155.300003</td>\n",
       "      <td>8.790000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.235935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>968.037476</td>\n",
       "      <td>15653.959961</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>156.699997</td>\n",
       "      <td>8.990000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.183978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>794.059998</td>\n",
       "      <td>45716.609375</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>157.500000</td>\n",
       "      <td>8.920001e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.736918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1087.806030</td>\n",
       "      <td>29580.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>9.540000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.677443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          chnimp       bchlimp  befile6  affile6  afdec6  befile12  affile12  \\\n",
       "0     220.462006   9578.375977        0        0       0         0         0   \n",
       "1      94.797997  11219.480469        0        0       0         0         0   \n",
       "2     219.357498   9719.900391        0        0       0         0         0   \n",
       "3     317.421509  12920.950195        0        0       0         0         0   \n",
       "4     114.639000   9790.446289        0        0       0         0         0   \n",
       "..           ...           ...      ...      ...     ...       ...       ...   \n",
       "126   589.421997  31686.859375        0        0       0         0         0   \n",
       "127   529.733521  16370.339844        0        0       0         0         0   \n",
       "128   968.037476  15653.959961        0        0       0         0         0   \n",
       "129   794.059998  45716.609375        0        0       0         0         0   \n",
       "130  1087.806030  29580.500000        0        0       0         0         0   \n",
       "\n",
       "     afdec12      chempi           gas  ...  apr  may  jun  jul  aug  sep  \\\n",
       "0          0  100.099998  7.830000e+09  ...    0    0    0    0    0    0   \n",
       "1          0  100.900002  8.820000e+09  ...    0    0    0    0    0    0   \n",
       "2          0  101.099998  8.450000e+09  ...    1    0    0    0    0    0   \n",
       "3          0  102.500000  9.240001e+09  ...    0    1    0    0    0    0   \n",
       "4          0  104.099998  9.150000e+09  ...    0    0    1    0    0    0   \n",
       "..       ...         ...           ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "126        0  154.800003  9.420000e+09  ...    0    0    0    0    1    0   \n",
       "127        0  155.300003  8.790000e+09  ...    0    0    0    0    0    1   \n",
       "128        0  156.699997  8.990000e+09  ...    0    0    0    0    0    0   \n",
       "129        0  157.500000  8.920001e+09  ...    0    0    0    0    0    0   \n",
       "130        0  159.000000  9.540000e+09  ...    0    0    0    0    0    0   \n",
       "\n",
       "     oct  nov  dec   percchn  \n",
       "0      0    0    0  2.301664  \n",
       "1      0    0    0  0.844941  \n",
       "2      0    0    0  2.256788  \n",
       "3      0    0    0  2.456642  \n",
       "4      0    0    0  1.170927  \n",
       "..   ...  ...  ...       ...  \n",
       "126    0    0    0  1.860147  \n",
       "127    0    0    0  3.235935  \n",
       "128    1    0    0  6.183978  \n",
       "129    0    1    0  1.736918  \n",
       "130    0    0    1  3.677443  \n",
       "\n",
       "[131 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregando os dados e criando a variável para o número de observações da série \n",
    "\n",
    "barium = woo.data('barium')\n",
    "T = len(barium) \n",
    "barium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:         np.log(chnimp)   R-squared:                       0.305\n",
      "Model:                            OLS   Adj. R-squared:                  0.271\n",
      "Method:                 Least Squares   F-statistic:                     9.064\n",
      "Date:                Tue, 06 Sep 2022   Prob (F-statistic):           3.25e-08\n",
      "Time:                        15:30:44   Log-Likelihood:                -114.79\n",
      "No. Observations:                 131   AIC:                             243.6\n",
      "Df Residuals:                     124   BIC:                             263.7\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================\n",
      "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "Intercept        -17.8028     21.045     -0.846      0.399     -59.457      23.852\n",
      "np.log(chempi)     3.1172      0.479      6.505      0.000       2.169       4.066\n",
      "np.log(gas)        0.1963      0.907      0.217      0.829      -1.598       1.991\n",
      "np.log(rtwex)      0.9830      0.400      2.457      0.015       0.191       1.775\n",
      "befile6            0.0596      0.261      0.228      0.820      -0.457       0.576\n",
      "affile6           -0.0324      0.264     -0.123      0.903      -0.556       0.491\n",
      "afdec6            -0.5652      0.286     -1.978      0.050      -1.131       0.001\n",
      "==============================================================================\n",
      "Omnibus:                        9.160   Durbin-Watson:                   1.458\n",
      "Prob(Omnibus):                  0.010   Jarque-Bera (JB):                9.978\n",
      "Skew:                          -0.491   Prob(JB):                      0.00681\n",
      "Kurtosis:                       3.930   Cond. No.                     9.62e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 9.62e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Série temporal mensal, selecionando o começo a partir de fevereiro de 1978\n",
    "\n",
    "barium.index = pd.date_range(start = '1978-02', periods = T, freq = 'M')\n",
    "\n",
    "# Regressão \n",
    "\n",
    "reg_barium = smf.ols('np.log(chnimp) ~ np.log(chempi) + np.log(gas) + np.log(rtwex) + befile6 + affile6 + afdec6',\n",
    "                     data = barium)\n",
    "results_reg_barium = reg_barium.fit()\n",
    "print(results_reg_barium.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teste do multiplicador de Lagrange (Breusch-Godfrey test)**: \n",
    "\n",
    "- Statsmodels: https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.acorr_breusch_godfrey.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F stat: 5.124662239772489\n",
      "\n",
      "pval: 0.0022637197671316546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computando os testes de autocorrelação (automaticamente):\n",
    "\n",
    "bg_test_result = acorr_breusch_godfrey(results_reg_barium, nlags = 3) # tupla com (lm, lmpval, fval, fpval)\n",
    "fstat_auto = bg_test_result[2]\n",
    "fpval_auto = bg_test_result[3]\n",
    "print(f'F stat: {fstat_auto}\\n')\n",
    "print(f'pval: {fpval_auto}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fstat_manual: 5.122907054069386\n",
      "\n",
      "fpval_manual: 0.002289802832966284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Teste com a regressão os resíduos:\n",
    "\n",
    "# (I) Calculando os resíduos e criando da primeira a terceira defasagem:\n",
    "\n",
    "barium['resid'] = results_reg_barium.resid\n",
    "barium['resid_lag1'] = barium['resid'].shift(1)\n",
    "barium['resid_lag2'] = barium['resid'].shift(2)\n",
    "barium['resid_lag3'] = barium['resid'].shift(3)\n",
    "\n",
    "# (II) Regressão do resíduo nas variáveis exógenas e nas suas três defasagens:\n",
    "\n",
    "reg_barium_resid = smf.ols('resid ~ resid_lag1 + resid_lag2 + resid_lag3 +'\n",
    "                           'np.log(chempi) + np.log(gas) + np.log(rtwex) +'\n",
    "                           'befile6 + affile6 + afdec6', \n",
    "                           data = barium)\n",
    "results_reg_barium_resid = reg_barium_resid.fit()\n",
    "\n",
    "# (III) Testando as hipóteses:\n",
    "\n",
    "hypothesis = ['resid_lag1 = 0', 'resid_lag2 = 0', 'resid_lag3 = 0']\n",
    "ftest_manual = results_reg_barium_resid.f_test(hypothesis)\n",
    "fstat_manual = ftest_manual.statistic\n",
    "fpval_manual = ftest_manual.pvalue\n",
    "print(f'fstat_manual: {fstat_manual}\\n')\n",
    "print(f'fpval_manual: {fpval_manual}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg.rho: [0.29585313]\n",
      "\n",
      "table: \n",
      "                   b_CORC    se_CORC\n",
      "Intercept      -37.512978  23.239015\n",
      "np.log(chempi)   2.945448   0.647696\n",
      "np.log(gas)      1.063321   0.991558\n",
      "np.log(rtwex)    1.138404   0.514910\n",
      "befile6         -0.017314   0.321390\n",
      "affile6         -0.033108   0.323806\n",
      "afdec6          -0.577328   0.344075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estimando por MQG com Cochrane-Orcutt (procedimento com interações):\n",
    "# OBS: No livro, usa-se a estimação Prais-Winsten, mas a dinâmica de ambas é similar e assintóticamente as diferenças são irrelevantes\n",
    "\n",
    "y, X = pt.dmatrices('np.log(chnimp) ~ np.log(chempi) + np.log(gas) +'\n",
    "                    'np.log(rtwex) + befile6 + affile6 + afdec6',\n",
    "                    data=barium, return_type='dataframe')\n",
    "reg = sm.GLSAR(y, X)\n",
    "CORC_results = reg.iterative_fit(maxiter = 100)\n",
    "table = pd.DataFrame({'b_CORC': CORC_results.params,\n",
    "                      'se_CORC': CORC_results.bse})\n",
    "print(f'reg.rho: {reg.rho}\\n')\n",
    "print(f'table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vale lembrar que nomralmente, tanto para a estimação Prais-Winsten quanto para a Cochrane-Orcutt os erros-padrões são maiores do que na estimação por MQO, dado que nos dois primeiros casos, considera-se a autocorrelação, enquanto em MQO ela não é computada na estimação. Isso ocorre porque **os estimadores de MQO geralmente subestimam a variação amostral dos dados com autocorrelação, produzindo erros-padrões menores.** \n",
    "\n",
    "> Nota-se que os parâmetros estatisticamente significantes não deixam de assumir essa condição com a mudança de método de estimação, dada a sua relevância para explicar o fenômeno. \n",
    "\n",
    "> O coeficiente de determinação $R^2$ também é inferior nas estimações Prais-Winsten e Cochrane-Orcutt em relação à regressão estimada por MQO. Contudo, a comparação é equivocada, pois o coeficiente de determinação para MQO é calculado com os dados sem tranformações, enquanto nos dois métodos citados acima os dados são tranformados. Ainda, para eles, é difícil estabelecer uma interpetação clara, dadas as tranformações feitas nos dados, mas mesmo assim o $R^2$ é reportado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 12.5: Static Phillips Curve - OLS X FGLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               Q(\"inf\")   R-squared:                       0.053\n",
      "Model:                            OLS   Adj. R-squared:                  0.033\n",
      "Method:                 Least Squares   F-statistic:                     2.616\n",
      "Date:                Tue, 06 Sep 2022   Prob (F-statistic):              0.112\n",
      "Time:                        15:30:45   Log-Likelihood:                -124.43\n",
      "No. Observations:                  49   AIC:                             252.9\n",
      "Df Residuals:                      47   BIC:                             256.6\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.4236      1.719      0.828      0.412      -2.035       4.882\n",
      "unem           0.4676      0.289      1.617      0.112      -0.114       1.049\n",
      "==============================================================================\n",
      "Omnibus:                        8.905   Durbin-Watson:                   0.803\n",
      "Prob(Omnibus):                  0.012   Jarque-Bera (JB):                8.336\n",
      "Skew:                           0.979   Prob(JB):                       0.0155\n",
      "Kurtosis:                       3.502   Cond. No.                         23.5\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# MQO:\n",
    "\n",
    "print(results_s.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg.rho: [0.79072939]\n",
      "\n",
      "table: \n",
      "             b_CORC   se_CORC\n",
      "Intercept  7.696532  2.453932\n",
      "unem      -0.689497  0.319084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MQGF:\n",
    "\n",
    "phillips_96 = phillips[phillips['year'] <= 1996]\n",
    "y, X = pt.dmatrices('Q(\"inf\") ~ unem', data = phillips_96, return_type='dataframe')\n",
    "reg = sm.GLSAR(y, X)\n",
    "CORC_results = reg.iterative_fit(maxiter = 100)\n",
    "table = pd.DataFrame({'b_CORC': CORC_results.params,\n",
    "                      'se_CORC': CORC_results.bse})\n",
    "print(f'reg.rho: {reg.rho}\\n')\n",
    "print(f'table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Podemos observar como os coeficientes são bruscamente diferentes: para a variável desemprego, muda-se inclusive o sinal, e não apenas o valor. Nesse caso, tendemos a ir com os coeficientes da estimação por MQGF (Cochrane-Orcutt), pois está de acordo com o sinal esperado pela teoria econômica, a Cruva de Phillips e o trade-off entre inflação e desemprego. \n",
    "\n",
    "> A estimação por MQGF é similar, em certo ponto, com o estimado por MQO com as diferenciações, obtendo a Curva de Phillips aceleracionista. Isso porque a estimação com MQGF utiliza de uma diferenciação parcial, que se aproxima da primeira diferenciação. Confira abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              inf_diff1   R-squared:                       0.108\n",
      "Model:                            OLS   Adj. R-squared:                  0.088\n",
      "Method:                 Least Squares   F-statistic:                     5.558\n",
      "Date:                Tue, 06 Sep 2022   Prob (F-statistic):             0.0227\n",
      "Time:                        15:30:45   Log-Likelihood:                -110.12\n",
      "No. Observations:                  48   AIC:                             224.2\n",
      "Df Residuals:                      46   BIC:                             228.0\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      3.0306      1.377      2.201      0.033       0.259       5.802\n",
      "unem          -0.5426      0.230     -2.357      0.023      -1.006      -0.079\n",
      "==============================================================================\n",
      "Omnibus:                       22.805   Durbin-Watson:                   1.770\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               52.071\n",
      "Skew:                          -1.239   Prob(JB):                     4.93e-12\n",
      "Kurtosis:                       7.460   Cond. No.                         23.9\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(results_ea.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Para esse caso, podemos observar que para processos não-estacionário, que possuem raiz unitária, a estimação por MQGF pode ser mais adequada que por MQO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparando MQO e MQGF\n",
    "\n",
    "Em termos práticos, e não apenas teóricos, os estimadores de MQO e MQGF podem ser bem diferentes.\n",
    "\n",
    "- Os estimadores de MQGF precisam que os dados em t, na sua primeira defasagem e na sua observação seguinte não sejam correlacionados com o termo do erro.\n",
    "\n",
    "- Geralmente, os estimadores podem ser bem diferentes para os dois métodos justamente porque a hipótese acima falha.\n",
    "\n",
    "- Nesse caso, temos duas alternativas:\n",
    "\n",
    "    - Se os coeficientes estimados forem muito distintos, devemos optar pelos estimadores de MQO, pois mesmo com autocorrelação, se as variáveis exógenas não estiverem correlacionadas com o termo do erro, teremos que os estimadores ainda serão consistentes assintóticamente. O mesmo não ocorre para MQGF, pois necessita da hipótese adicional colocada acima, que geralmente falha.\n",
    "\n",
    "    - Se os coeficientes estimados forem similares podemos assumir que a hipótese acima apresentada se mantém de alguma forma, e por isso os estimadores de MQGF são preferíveis por serem mais eficientes sob presença de forte evidência para a correlação serial.\n",
    "\n",
    "- Ainda, podemos testar se a diferenças dos estimadores é realmente significante do ponto de vista estatístico, usando o teste de Hausmann, que vimos para estimadores com efeitos fixos e aleatórios para dados em painel.\n",
    "\n",
    "## Corrigindo para correlação serial de ordens superiores\n",
    "\n",
    "- Assumindo condições de estabilidade, podemos tranformar os dados para eliminar a correlação serial, como feito no caso mais simples.\n",
    "\n",
    "- Tomamos os mesmo procedimentos:\n",
    "    - Regressão com MQO e extração dos resíduos;\n",
    "\n",
    "    - Regressão dos resíduos em suas defasagens e nas variáveis independentes;\n",
    "\n",
    "    - Obtenção dos rhos e dos testes significância\n",
    "\n",
    "    - Transformação dos dados e nova regressão com MQO.\n",
    "\n",
    "- Esse procedimento é análogo a estimação de MQGF e eliminam a correlação serial para o número de observações escolhido (2 ou mais) e tornamos erros homoscedasticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vale lembrar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Processo integrado de ordem zero **I(0)** --> Processo/série estacionária ou com dependência fraca;\n",
    "\n",
    "2. Processo integrado de ordem um **I(1)** --> Processo/série não-estacionária, mas que sua primeira diferenciação transforma-se em uma série com dependência fraca (ou estacionária);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diferenciação (decomposição) e Correlação Serial\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os processos de diferenciação (decomposição) também possuem méritos quando lidamos com séries persistentes, dada a sua capacidade de tornar processos integrados em processos de dependência fraca. \n",
    "\n",
    "- A inferência com os estimadores de MQO sem a diferenciação dos dados pode nos levar a conclusões erradas, principalmente se os processos são integrados em primeira ordem I(1) ou superiores. O passeio aleatório é um dos exemplos.\n",
    "\n",
    "- Assim, a **diferenciação é uma ferramenta interessante para eliminar a correlação serial ou parte dela.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 12.6: Differencing the Interest Rate Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "intdef = woo.data('intdef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     i3   R-squared:                       0.602\n",
      "Model:                            OLS   Adj. R-squared:                  0.587\n",
      "Method:                 Least Squares   F-statistic:                     40.09\n",
      "Date:                Tue, 06 Sep 2022   Prob (F-statistic):           2.48e-11\n",
      "Time:                        15:30:46   Log-Likelihood:                -112.16\n",
      "No. Observations:                  56   AIC:                             230.3\n",
      "Df Residuals:                      53   BIC:                             236.4\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.7333      0.432      4.012      0.000       0.867       2.600\n",
      "Q(\"inf\")       0.6059      0.082      7.376      0.000       0.441       0.771\n",
      "Q(\"def\")       0.5131      0.118      4.334      0.000       0.276       0.751\n",
      "==============================================================================\n",
      "Omnibus:                        0.260   Durbin-Watson:                   0.716\n",
      "Prob(Omnibus):                  0.878   Jarque-Bera (JB):                0.015\n",
      "Skew:                          -0.028   Prob(JB):                        0.992\n",
      "Kurtosis:                       3.058   Cond. No.                         9.28\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Regressão OLS\n",
    "\n",
    "irate_regOLS = smf.ols(formula='i3 ~ Q(\"inf\") + Q(\"def\")', data = intdef)\n",
    "results_irate_regOLS = irate_regOLS.fit()\n",
    "print(results_irate_regOLS.summary())\n",
    "intdef['resid'] = results_irate_regOLS.resid\n",
    "intdef['resid_lag1'] = intdef['resid'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  resid   R-squared:                       0.377\n",
      "Model:                            OLS   Adj. R-squared:                  0.366\n",
      "Method:                 Least Squares   F-statistic:                     32.13\n",
      "Date:                Tue, 06 Sep 2022   Prob (F-statistic):           6.06e-07\n",
      "Time:                        15:30:46   Log-Likelihood:                -95.938\n",
      "No. Observations:                  55   AIC:                             195.9\n",
      "Df Residuals:                      53   BIC:                             199.9\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.0153      0.190      0.081      0.936      -0.366       0.397\n",
      "resid_lag1     0.6225      0.110      5.669      0.000       0.402       0.843\n",
      "==============================================================================\n",
      "Omnibus:                        2.974   Durbin-Watson:                   1.593\n",
      "Prob(Omnibus):                  0.226   Jarque-Bera (JB):                2.247\n",
      "Skew:                           0.217   Prob(JB):                        0.325\n",
      "Kurtosis:                       3.890   Cond. No.                         1.74\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Regressão com os resíduos para verificar a correlação serial\n",
    "\n",
    "reg_irate_resid = smf.ols('resid ~ resid_lag1', data = intdef)\n",
    "results_reg_irate_resid = reg_irate_resid.fit()\n",
    "print(results_reg_irate_resid.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O coeficiente $\\rho_1$ para a primeira defasagem do resíduo indica a presença de forte correlação serial, o que nos leva a necessidade de corrigir o problema. Vamos utilizar da diferenciação para contornar a autocorrelação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "intdef['i3_diff1'] = intdef['i3'].diff()\n",
    "intdef['inf_diff1'] = intdef['inf'].diff()\n",
    "intdef['def_diff1'] = intdef['def'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     i3   R-squared:                       0.602\n",
      "Model:                            OLS   Adj. R-squared:                  0.587\n",
      "Method:                 Least Squares   F-statistic:                     40.09\n",
      "Date:                Tue, 06 Sep 2022   Prob (F-statistic):           2.48e-11\n",
      "Time:                        15:30:46   Log-Likelihood:                -112.16\n",
      "No. Observations:                  56   AIC:                             230.3\n",
      "Df Residuals:                      53   BIC:                             236.4\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.7333      0.432      4.012      0.000       0.867       2.600\n",
      "Q(\"inf\")       0.6059      0.082      7.376      0.000       0.441       0.771\n",
      "Q(\"def\")       0.5131      0.118      4.334      0.000       0.276       0.751\n",
      "==============================================================================\n",
      "Omnibus:                        0.260   Durbin-Watson:                   0.716\n",
      "Prob(Omnibus):                  0.878   Jarque-Bera (JB):                0.015\n",
      "Skew:                          -0.028   Prob(JB):                        0.992\n",
      "Kurtosis:                       3.058   Cond. No.                         9.28\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               i3_diff1   R-squared:                       0.176\n",
      "Model:                            OLS   Adj. R-squared:                  0.145\n",
      "Method:                 Least Squares   F-statistic:                     5.566\n",
      "Date:                Tue, 06 Sep 2022   Prob (F-statistic):            0.00645\n",
      "Time:                        15:30:46   Log-Likelihood:                -89.416\n",
      "No. Observations:                  55   AIC:                             184.8\n",
      "Df Residuals:                      52   BIC:                             190.9\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.0418      0.171      0.244      0.808      -0.302       0.386\n",
      "inf_diff1      0.1495      0.092      1.622      0.111      -0.035       0.334\n",
      "def_diff1     -0.1813      0.148     -1.228      0.225      -0.478       0.115\n",
      "==============================================================================\n",
      "Omnibus:                        0.777   Durbin-Watson:                   1.796\n",
      "Prob(Omnibus):                  0.678   Jarque-Bera (JB):                0.876\n",
      "Skew:                           0.203   Prob(JB):                        0.645\n",
      "Kurtosis:                       2.534   Cond. No.                         2.68\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Rodando novamente a regressão com os dados diferenciados\n",
    "\n",
    "irate_regdiff = smf.ols(formula = 'i3_diff1 ~ inf_diff1 + def_diff1', data = intdef)\n",
    "results_irate_regdiff = irate_regdiff.fit()\n",
    "print(results_irate_regOLS.summary(), results_irate_regdiff.summary(), sep = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Podemos visualizar que os coeficientes são bem diferentes entre as regressões com os dados originais em nível e os dados diferenciados. Isso é uma evidência de que as variáveis exógenas não são estritamente exógenas, trazendo o problema da endogeneidade, ou que as variáveis apresentam raiz unitária. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            resid_diff1   R-squared:                       0.005\n",
      "Model:                            OLS   Adj. R-squared:                 -0.014\n",
      "Method:                 Least Squares   F-statistic:                    0.2871\n",
      "Date:                Tue, 06 Sep 2022   Prob (F-statistic):              0.594\n",
      "Time:                        15:30:46   Log-Likelihood:                -86.478\n",
      "No. Observations:                  54   AIC:                             177.0\n",
      "Df Residuals:                      52   BIC:                             180.9\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "====================================================================================\n",
      "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept           -0.0414      0.166     -0.249      0.805      -0.375       0.293\n",
      "resid_diff1_lag1     0.0719      0.134      0.536      0.594      -0.197       0.341\n",
      "==============================================================================\n",
      "Omnibus:                        0.306   Durbin-Watson:                   1.964\n",
      "Prob(Omnibus):                  0.858   Jarque-Bera (JB):                0.431\n",
      "Skew:                           0.159   Prob(JB):                        0.806\n",
      "Kurtosis:                       2.700   Cond. No.                         1.24\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Conferindo a correlação serial da nova regressão\n",
    "\n",
    "intdef['resid_diff1'] = results_irate_regdiff.resid\n",
    "intdef['resid_diff1_lag1'] = intdef['resid_diff1'].shift(1)\n",
    "\n",
    "irate_regdiff_resid = smf.ols(formula = 'resid_diff1 ~ resid_diff1_lag1', data = intdef)\n",
    "results_irate_regdiff_resid = irate_regdiff_resid.fit()\n",
    "print(results_irate_regdiff_resid.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Concluímos que eliminamos a correlação serial dos erros dado o pequeno coeficiente $\\rho_1$ da lag do resíduo e graças ao resultado do teste t, que nos dá mais e 50% de chance de erro ao rejeitarmos a hipótese nula de que não há autocorrelação. \n",
    "\n",
    "> Ou seja, **a diferenciação nos auxiliou a retiar a raiz unitária das séries analisadas, tornado-as estacionárias, e nos ajudou a controlar a correlação serial.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlação serial e Inferência Robsuta após MQO\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por alguns dos impasses citados com o MQGF, tornou-se mais popular estimar os modelos por MQO mesmo com correlação serial, corringindo esse problema e o da heteroscedasticidade com o uso dos erros-padrões robustos dos estimadores.\n",
    "\n",
    "Os **estimadores de MQO serão ineficientes**, mas algumas das explicações para essa abordagem são:\n",
    "\n",
    "1. Na maioria das aplicações, as variáveis não são estritamente exógenas, o que deixa MQGF inferior ao MQO, pois nesse caso, não serão nem eficientes e nem consistentes;\n",
    "\n",
    "2. Em MQGF os erros são assumidos como um processo autorregressivo de ordem 1 AR(1). Em MQO podemos generalizar com os erros-padrões robustos para outras formas.\n",
    "\n",
    "3. No caso de MQO, para calcularmos a varância dos erros, basta plugar $\\rho$ na relação derivada no livro texto, mas aqui assumimos que há homoscedasticidade. Mas essa hipótese pode ser relaxada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lidando com erros heteroscedasticos e autoccorelacionados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abordagem robusta desenvolvida por **Davidson e MacKinnon (1993)**.\n",
    "\n",
    "1. Estimamos a regressão da maneira convencional;\n",
    "\n",
    "2. Prosseguimos com uma regressão de uma das variáveis exógenas contra as demais exógenas (podemos repetir o processo para mais de uma, se necessário);\n",
    "\n",
    "3. Então, podemos mostrar que a variância assintótica robusta do estimador depende do erro dos passos 1 e 2 acima. \n",
    "\n",
    "$$\n",
    "AVar(\\beta_1) = (\\sum_{t = 1}^{n} E(r_{t}^2))^{-2} Var(\\sum_{t = 1}^{n} r_t u_t)\n",
    "$$\n",
    "\n",
    "**Com ou sem correlação serial, chegaremos ao erro padrão robusto que serve para ambos os casos.** Então, consideramos que quando a correlação não é persistente para muitos períodos a frente, conseguimos fazer a inferência robusta controlando a correlação para períodos curtos com essa relação. Então, a dependência fraca é interessante para esse tipo de problema, dado que espera-se que nessas condições a correlação serial reduza rapidamente para períodos curtos. \n",
    "\n",
    "Com isso, Newey e West (1987) e Wooldridge (1989) indicam uma forma de estimar a $AVar$ e o erro-padrão dos estimadores a partir da relação acima. \n",
    "- **[VERIFICAR DERIVAÇÃO COMPLETA NO LIVRO]**\n",
    "\n",
    "A partir desses procedimentos, temos erros-padrões robustos com viabilidade para a relaização dos testes $t$ e $F$. Nesse caso, a literatura de séries temporais os chama de *Erro-padrão consistente à autocorrelação e heteroscedasticidade*, em inglês: ***heteroskedasticity and autocorrelation consistent standard errors (HAC standard errors)***.\n",
    "\n",
    "Podemos ajustar essa relação para diferentes tipos de correlação serial e diferentes tamanhos de amostra. Além disso, dependendo do tamanho da amostra podemos ser mais flexíveis em relação à correlação serial. Newey e West (1987) recomendam a inclusão do número de termos para a correlação serial $g$ como:\n",
    "\n",
    "$$\n",
    "4(n/100)^{2/9}\n",
    "$$\n",
    "\n",
    "Outra \"regra\" seria:\n",
    "\n",
    "$$\n",
    "n^{1/4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedimento para computar o erro padrão para um estimador qualquer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Estimar a regressão por MQO normalmente;\n",
    "\n",
    "2. Computar os resíduos da regressão auxiliar (regressão da variável independente correspondente ao estimador analisado nas demais variáveis independentes) e incorporar um termo de erro composto $\\hat{a_t} = \\hat{r_t}\\hat{u_t}$ para cada t, sendo r e u os erros da regressão auxiliar e da regressão inicial, respectivamente.\n",
    "\n",
    "3. Escolher $g$ por uma das regras listadas acima e computar $\\hat{v}$, o erro padrão robusto à correlação serial;\n",
    "\n",
    "4. Computar o erro padrão do estimador pela regra dada no livro texto, equação [12.43]\n",
    "\n",
    "### Diferenças entre o erro padrão e o erro padrão robusto e impasses\n",
    "\n",
    "Além das diferenças teóricas já citadas, pelo erro padrão vir de um processo de estimação que ignora a correlação serial, espera-se, na maioria dos casos, que empiricamente o erro padrão robusto seja maior.  \n",
    "\n",
    "Vale lembrar que quando temos correlação serial substancial somada a um tamanho pequeno de amostra, os erros padrões robustos podem performar mal. Dependendo da intensidade da correlação serial, temos que os coeficientes podem ser menos significativos do que pensamos, do ponto de vista estatístico, ou mesmo não significantes. \n",
    "\n",
    "Se estivermos confiantes em relação a exogeneidade estrita das variáveis independentes, por mais que seja uma hipótese forte, e sejamos céticos em relação aos erros seguirem um processo AR(1), podemos estimar por MQGF com Prais-Winsten ou Cochrane-Orcutt. Isso porque com o processo de decomposição parcial desses métodos ainda podemos obter estimadores do que em MQO, sob as condições descritas acima. Mas se os erros de fato não seguirem um processo AR(1), então os erros padrões estimados estarão incorretos. \n",
    "\n",
    "Em outros casos, podemos ainda decompor parcialmente a série de forma manual, estimar por MQO e então calcular os erros padrões robustos. Esse procedimento elimina boa parte da correlação serial e é semelhante ao que fazemos com Mínimos Quadrados Ponderados na presença de heteroscedasticidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***DOCUMENTAÇÃO E EXEMPLOS DE COMO IMPLEMENTAR ERRO-PADRÃO ROBUSTO:*** https://stackoverflow.com/questions/30553838/getting-statsmodels-to-use-heteroskedasticity-corrected-standard-errors-in-coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heteroscedasticidade em Séries Temporais\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como como em dados cross-section, a heteroscedasticidade não provoca viés ou incosistência nos estimadores, mas **torna inválidos os erros padrões, as estatísticas t e F**, inviabilizando a inferência. \n",
    "\n",
    "Geralmente, temos que os problemas de correlação serial são mais relevantes do que os problemas de heteroscedasticidade em séries temporais, isso porque algumas ferramentas que usamos para corrigir a correlação serial podem corrigir a heteroscedasticidade, mas o contrário não ocorre. Ainda, os problemas vindos da correlação serial são mais relevantes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em relação as correções, podemos seguir com os seguintes procedimentos:\n",
    "\n",
    "1. Para corrigir as estatísticas de teste, basta seguirmos o mesmo procedimento do que fizemos com dados cross-section, utilizando os erros padrões e as estatísticas de teste robustas à heteroscedasticidade. Os pacotes econométricos fornecem com facilidade.\n",
    "\n",
    "As vezes, queremos testar para a heteroscedasticidade em aplicações expecíficas, principalmente quando estamos lidando com pequenas amostras. Mas vale lembrar que para testarmos para heteroscedasticidade, a **série não poder conter correlação serial, caso contrário, o teste será invalidado**. Dessa forma, faz sentido **testar para correlação serial antes de testar para heteroscedasticidade**.\n",
    "\n",
    "Dessa forma:\n",
    "\n",
    "2. Podemos conduzir o teste de Breusch-Pagan, com a regressão dos resíduos ao quadrado nas variáveis independentes da regressão inicial. Vale lembrar que o erro dessa regressão deve ser homoscedastico e não correlacionado serialmente. \n",
    "\n",
    "Esse procedimento deve ser considerado padrão para outros testes também, como o teste de White. Ainda, se houver heteroscedasticidade no erro e não houver correlação serial, podemos prosseguir com as estatísticas de teste robustas à heteroscedasticidade.  \n",
    "\n",
    "3. Outra alternativa é conduzir uma regressão com estimadores de mínimos quadrados ponderados (MQP ou WLS em inglês) e comparar os estimadores e as estatísticas de teste com MQO. Esse procedimento é comum ao que já realizamos para dados cross section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 12.8: Heteroskedasticity and the Efficient Markets Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>return</th>\n",
       "      <th>return_1</th>\n",
       "      <th>t</th>\n",
       "      <th>price_1</th>\n",
       "      <th>price_2</th>\n",
       "      <th>cprice</th>\n",
       "      <th>cprice_1</th>\n",
       "      <th>ret</th>\n",
       "      <th>ret_lag1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.439999</td>\n",
       "      <td>3.396982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>49.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.689999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.396982</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.049999</td>\n",
       "      <td>1.185849</td>\n",
       "      <td>3.396982</td>\n",
       "      <td>3</td>\n",
       "      <td>51.439999</td>\n",
       "      <td>49.750000</td>\n",
       "      <td>0.610001</td>\n",
       "      <td>1.689999</td>\n",
       "      <td>1.185849</td>\n",
       "      <td>3.396982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52.279999</td>\n",
       "      <td>0.441882</td>\n",
       "      <td>1.185849</td>\n",
       "      <td>4</td>\n",
       "      <td>52.049999</td>\n",
       "      <td>51.439999</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.610001</td>\n",
       "      <td>0.441882</td>\n",
       "      <td>1.185849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54.240002</td>\n",
       "      <td>3.749049</td>\n",
       "      <td>0.441882</td>\n",
       "      <td>5</td>\n",
       "      <td>52.279999</td>\n",
       "      <td>52.049999</td>\n",
       "      <td>1.960003</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>3.749049</td>\n",
       "      <td>0.441882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>161.740005</td>\n",
       "      <td>-1.106694</td>\n",
       "      <td>-1.004784</td>\n",
       "      <td>687</td>\n",
       "      <td>163.550003</td>\n",
       "      <td>165.210007</td>\n",
       "      <td>-1.809998</td>\n",
       "      <td>-1.660004</td>\n",
       "      <td>-1.106694</td>\n",
       "      <td>-1.004784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>165.449997</td>\n",
       "      <td>2.293799</td>\n",
       "      <td>-1.106694</td>\n",
       "      <td>688</td>\n",
       "      <td>161.740005</td>\n",
       "      <td>163.550003</td>\n",
       "      <td>3.709991</td>\n",
       "      <td>-1.809998</td>\n",
       "      <td>2.293799</td>\n",
       "      <td>-1.106694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>166.669998</td>\n",
       "      <td>0.737384</td>\n",
       "      <td>2.293799</td>\n",
       "      <td>689</td>\n",
       "      <td>165.449997</td>\n",
       "      <td>161.740005</td>\n",
       "      <td>1.220001</td>\n",
       "      <td>3.709991</td>\n",
       "      <td>0.737384</td>\n",
       "      <td>2.293799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>163.410004</td>\n",
       "      <td>-1.955958</td>\n",
       "      <td>0.737384</td>\n",
       "      <td>690</td>\n",
       "      <td>166.669998</td>\n",
       "      <td>165.449997</td>\n",
       "      <td>-3.259995</td>\n",
       "      <td>1.220001</td>\n",
       "      <td>-1.955958</td>\n",
       "      <td>0.737384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>164.250000</td>\n",
       "      <td>0.514042</td>\n",
       "      <td>-1.955958</td>\n",
       "      <td>691</td>\n",
       "      <td>163.410004</td>\n",
       "      <td>166.669998</td>\n",
       "      <td>0.839996</td>\n",
       "      <td>-3.259995</td>\n",
       "      <td>0.514042</td>\n",
       "      <td>-1.955958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>691 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          price    return  return_1    t     price_1     price_2    cprice  \\\n",
       "0     49.750000       NaN       NaN    1         NaN         NaN       NaN   \n",
       "1     51.439999  3.396982       NaN    2   49.750000         NaN  1.689999   \n",
       "2     52.049999  1.185849  3.396982    3   51.439999   49.750000  0.610001   \n",
       "3     52.279999  0.441882  1.185849    4   52.049999   51.439999  0.230000   \n",
       "4     54.240002  3.749049  0.441882    5   52.279999   52.049999  1.960003   \n",
       "..          ...       ...       ...  ...         ...         ...       ...   \n",
       "686  161.740005 -1.106694 -1.004784  687  163.550003  165.210007 -1.809998   \n",
       "687  165.449997  2.293799 -1.106694  688  161.740005  163.550003  3.709991   \n",
       "688  166.669998  0.737384  2.293799  689  165.449997  161.740005  1.220001   \n",
       "689  163.410004 -1.955958  0.737384  690  166.669998  165.449997 -3.259995   \n",
       "690  164.250000  0.514042 -1.955958  691  163.410004  166.669998  0.839996   \n",
       "\n",
       "     cprice_1       ret  ret_lag1  \n",
       "0         NaN       NaN       NaN  \n",
       "1         NaN  3.396982       NaN  \n",
       "2    1.689999  1.185849  3.396982  \n",
       "3    0.610001  0.441882  1.185849  \n",
       "4    0.230000  3.749049  0.441882  \n",
       "..        ...       ...       ...  \n",
       "686 -1.660004 -1.106694 -1.004784  \n",
       "687 -1.809998  2.293799 -1.106694  \n",
       "688  3.709991  0.737384  2.293799  \n",
       "689  1.220001 -1.955958  0.737384  \n",
       "690 -3.259995  0.514042 -1.955958  \n",
       "\n",
       "[691 rows x 10 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyse = woo.data('nyse')\n",
    "nyse['ret'] = nyse['return']\n",
    "nyse['ret_lag1'] = nyse['return_1']\n",
    "nyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               resid_sq   R-squared:                       0.042\n",
      "Model:                            OLS   Adj. R-squared:                  0.041\n",
      "Method:                 Least Squares   F-statistic:                     30.05\n",
      "Date:                Tue, 06 Sep 2022   Prob (F-statistic):           5.90e-08\n",
      "Time:                        15:30:47   Log-Likelihood:                -2639.9\n",
      "No. Observations:                 689   AIC:                             5284.\n",
      "Df Residuals:                     687   BIC:                             5293.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      4.6565      0.428     10.888      0.000       3.817       5.496\n",
      "ret_lag1      -1.1041      0.201     -5.482      0.000      -1.500      -0.709\n",
      "==============================================================================\n",
      "Omnibus:                     1296.711   Durbin-Watson:                   1.443\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1627670.115\n",
      "Skew:                          12.811   Prob(JB):                         0.00\n",
      "Kurtosis:                     239.728   Cond. No.                         2.14\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Regressão principal\n",
    "\n",
    "heteros_efh_reg = smf.ols('ret ~ ret_lag1', data = nyse)\n",
    "results_heteros_efh_reg = heteros_efh_reg.fit()\n",
    "\n",
    "# Obtendo os resíduos ao quadrado\n",
    "\n",
    "nyse['resid'] = results_heteros_efh_reg.resid\n",
    "nyse['resid_sq'] = nyse['resid'] ** 2\n",
    "\n",
    "# Regressão para teste de heteroscedasticidade\n",
    "\n",
    "u_sq_reg = smf.ols('resid_sq ~ ret_lag1', data = nyse)\n",
    "results_u_sq_reg = u_sq_reg.fit()\n",
    "print(results_u_sq_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heteroscedasticidade condicional autorregressiva (Autoregressive Conditional Heteroskedasticity, ARCH)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A heteroscedasticidade é um dos pontos de atenção nas regressões e podemos verificá-la em sua forma dinâmica até em modelos sem dinâmica na equação da regressão.\n",
    "\n",
    "O modelo ARCH de primeira ordem é dado por:\n",
    "\n",
    "$$\n",
    "E(u_t^2|u_{t-1}, u_{t-1}, ...) = E(u_t^2|u_{t-1}) = \\alpha_0 + \\alpha_1u_{t-1}^2 + v_t\n",
    "$$\n",
    "\n",
    "se $\\alpha_1 = 0$, não há dinâmica e o modelo não faz sentido. Logo, $\\alpha_1 \\geq 0$ e $\\alpha_0 > 0$ \n",
    "\n",
    "Ainda, os termos do erro ao quadrado na regressão possuem correlação serial, mesmo que nasua forma linear não haja correlação serial.\n",
    "\n",
    "- Condição de estabilidade $\\alpha_1 < 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quais as implicações para os estimadores de MQO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se as hipóteses Gauss-Markov se mantiverem, teremos que os estimadores de MQO ainda serão consistentes e suas estatísticas de testes serão válidas.\n",
    "\n",
    "Contudo, mesmo com as propriedades desejadas, ainda devemos nos preocupar com duas situações no caso de estimadores de MQO:\n",
    "\n",
    "1. Podemos obter estimadores assintóticamente mais eficientes e consistentes (embora não necessariamente não-viesados): como por MQP ou máxima verossimilhança se os erros se distribuírem normalmente.\n",
    "\n",
    "2. Em alguns campos da economia a variância condicional, associada ao erro, é relevante. Como a variância é utilizada para medir volatilidade, os moelos ARCH ganharam destaque principalmente na área das finanças. \n",
    "\n",
    "Ainda, vale lembrar que o modelo ARCH também se aplica para a dinâmica na média condicional. Mas novamente, sob as hipótese Gauss-Markov MQO ainda terá as propriedades desejadas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 12.9: ARCH in Stock Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               resid_sq   R-squared:                       0.114\n",
      "Model:                            OLS   Adj. R-squared:                  0.112\n",
      "Method:                 Least Squares   F-statistic:                     87.92\n",
      "Date:                Tue, 06 Sep 2022   Prob (F-statistic):           9.71e-20\n",
      "Time:                        16:04:07   Log-Likelihood:                -2609.7\n",
      "No. Observations:                 688   AIC:                             5223.\n",
      "Df Residuals:                     686   BIC:                             5233.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=================================================================================\n",
      "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "Intercept         2.9474      0.440      6.695      0.000       2.083       3.812\n",
      "resid_sq_lag1     0.3371      0.036      9.377      0.000       0.266       0.408\n",
      "==============================================================================\n",
      "Omnibus:                     1343.910   Durbin-Watson:                   2.028\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2176433.255\n",
      "Skew:                          13.807   Prob(JB):                         0.00\n",
      "Kurtosis:                     277.152   Cond. No.                         13.2\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Criando a primeira defasagem do erro ao quadrado\n",
    "\n",
    "nyse['resid_sq_lag1'] = nyse['resid_sq'].shift(1)\n",
    "\n",
    "# ARCH model nos retornos das ações\n",
    "\n",
    "arch_reg = smf.ols('resid_sq ~ resid_sq_lag1', data = nyse)\n",
    "results_arch_reg = arch_reg.fit()\n",
    "print(results_arch_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Com o coeficiente da primeira defasagem do resíduo ao quadrado e sua estatística de teste, podemos ver que há forte evidência para a heteroscedasticidade dinâmica e a volatilidade de hoje depende da volatilidade passada. Os resíduos ao quadrado da regressão possuem correlação serial, mas os resíduos em sua forma comum não, o que também é uma evidênica, até certo ponto, da validade da hipóstese dos mercados eficientes ao longo do tempo. O retorno de hoje não está correlacionado com o passad, mas a variância nos retornos de hoje está correlacionada com a variância no passado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heteroscedasticidade e Correlação serial em modelos de regressão\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale destacar também a possibilidade de ocorrência de heteroscedasticidade e correlação serial, juntas. Sem a certeza absoluta, podemos sempre prosseguir com MQO computandos os erros padrões robustos. \n",
    "\n",
    "Vale ressaltar que, geralmente, a correlação serial é considerada como o maior problema nesses casos, pois seu impacto sobre a consistência e a validade das estatisticas de testes são mais relevantes.\n",
    "\n",
    "Para esse problema mais relevante, podemos prosseguir com os testes de identificação robustos e, se for o caso, utilizar as estimações por e Cochrane-Orcutt ou Prais-Winsten com a decomposição parcial da série, ou podemos realizar uma decomposição e estimar por MQO usando as estatísticas robustas de teste. Ou podemos realizar os testes de Breusch-Pagan ou White. \n",
    "\n",
    "Podemos ainda corrigir ambos os problemas de uma só vez, com a estimação por Mínimos Quadrados Ponderados junto da abordagem AR(1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MQGF e AR(1) com correlação serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na maioria dos casos, devemos prosseguir com os seguintes passos:\n",
    "\n",
    "1. Estimar por MQO e armazenar os resíduos;\n",
    "\n",
    "2. Regredir $log(hat{u_t^2}$ nas variáveis exógenas ou na endógena normal e ao quadrado e obter os valores ajustados g;\n",
    "\n",
    "3. Obter as estimativas de h com a forma $hat{h_t} = exp(g_t)$\n",
    "\n",
    "4. Estimar a equação transformada com CO ou PW. \n",
    "\n",
    "Assim, teremos estimadores eficientes e as estatísticas de teste serão válidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50b6e48135b3aa07383f5a96ff512fd2c607c6638df435f7ef5dee28b012c53c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
